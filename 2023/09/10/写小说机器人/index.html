<!DOCTYPE html>

<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Steve Hu的博客 | 写小说机器人</title>

    
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="author" content="Steve Hu">

    <meta name="color-scheme" content="light dark">

    
    
<link rel="stylesheet" href="/styles/index.css">

    
    <link rel="shortcut icon" href="/imgs/logo.png">

    
    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
    
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/styles/base16/solarized-light.css">
    

    
<meta name="generator" content="Hexo 6.3.0"></head>


<body id="delicate-app">
    <main>
        <div class="container">
            <section class="page-top-badge flex-between w100">
                <div>
                    <a href="/">
                        <span title="github"><i class="fa fa-home" aria-hidden="true"></i></span>
                    </a>
                </div>
                <!-- <div>
                    
                </div> -->
            </section>
            <aside class="toc-container">
    <!-- sticky toc -->
    
</aside>

<article class="markdown-container">
    <section>
        <h1>
            <span>
                写小说机器人
            </span>
        
            <div class="post-info">
                <span></span>
                <span>
                    18k字 • 
                    36分钟
                </span>
            </div>
        </h1>
    </section>

    <section class="article">
        <p>胡哲涵最新开源项目，小说大师python源码</p>
<span id="more"></span>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">                                                          声明</span></span><br><span class="line"><span class="code">本项目基于pytorch实现。可以自行更换训练文本，但请记住：训练文本一定一定得是TXT格式！！！</span></span><br><span class="line"><span class="code">另外，本项目仅用作学习交流，严禁用于商用、参加任何形式以及团体组织的比赛，请使用者严格遵守Apache License Version 开源协议</span></span><br><span class="line"><span class="code">作者不承担此程序造成的一切后果与责任。</span></span><br><span class="line"><span class="code">版权归作者重庆市小学生胡哲涵所有，Github likehuiyuanai。</span></span><br><span class="line"><span class="code">PS：最近喜欢看某科学的超电磁炮，所以实例代码用的是日本轻小说《魔法禁书目录》作为训练数据，暂拒绝公开此数据集（你可以自己整理，字数多亿些训练效果好点</span></span><br><span class="line"><span class="code">本博客（帖子）的版权也同样归胡哲涵所有，严禁转发或参加比赛！</span></span><br><span class="line"><span class="code">低调！低调！低调！</span></span><br></pre></td></tr></table></figure>
<p>正式开始（暂时没有开发代码生成器的意向，毕竟我不喜欢。没有bug的代码等同于人失去了灵魂！）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>导入库，幼儿园都学过吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./mfjsml.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.readlines()</span><br></pre></td></tr></table></figure>
<p>读取数据集，mfjsml这个名字可以改，但数据集和jupyter lab代码放一个文件夹下！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=<span class="string">&#x27;&#x27;</span>.join(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data[:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>
<p>展示数据集的一部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chars = <span class="built_in">list</span>(<span class="built_in">set</span>(data))</span><br><span class="line">data_size, vocab_size = <span class="built_in">len</span>(data), <span class="built_in">len</span>(chars)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;data has <span class="subst">&#123;data_size&#125;</span> characters, <span class="subst">&#123;vocab_size&#125;</span> unique.&#x27;</span>)</span><br><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125;</span><br><span class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125;</span><br></pre></td></tr></table></figure>
<p>接下来我们开始构建LSTM模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train = csr_matrix((<span class="built_in">len</span>(data), <span class="built_in">len</span>(chars)), dtype=np.<span class="built_in">int</span>)</span><br><span class="line">char_id = np.array([chars.index(c) <span class="keyword">for</span> c <span class="keyword">in</span> data])</span><br><span class="line">X_train[np.arange(<span class="built_in">len</span>(data)), char_id] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.roll(char_id,-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">X_train, y_train, seq_length</span>):</span><br><span class="line">    X = X_train</span><br><span class="line">    <span class="comment">#X = torch.from_numpy(X_train).float()</span></span><br><span class="line">    y = torch.from_numpy(y_train).long()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y), seq_length):   </span><br><span class="line">        id_stop = i+seq_length <span class="keyword">if</span> i+seq_length &lt; <span class="built_in">len</span>(y) <span class="keyword">else</span> <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">yield</span>([torch.from_numpy(X[i:id_stop].toarray().astype(np.float32)), </span><br><span class="line">               y[i:id_stop]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_chars</span>(<span class="params">rnn, X_seed, h_prev, length=<span class="number">20</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Generate text using trained model&#x27;&#x27;&#x27;</span></span><br><span class="line">    X_next = X_seed</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):        </span><br><span class="line">            y_score, h_prev = rnn(X_next.view(<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>), h_prev)</span><br><span class="line">            y_prob = nn.Softmax(<span class="number">0</span>)(y_score.view(-<span class="number">1</span>)).detach().numpy()</span><br><span class="line">            y_pred = np.random.choice(chars,<span class="number">1</span>, p=y_prob).item()</span><br><span class="line">            results.append(y_pred)</span><br><span class="line">            X_next = torch.zeros_like(X_seed)</span><br><span class="line">            X_next[chars.index(y_pred)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(results)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">nn_LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size)</span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, hidden</span>):</span><br><span class="line">        _, hidden = self.lstm(X, hidden)</span><br><span class="line">        output = self.out(hidden[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initHidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size),</span><br><span class="line">                torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size)</span><br><span class="line">               )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">seq_length = <span class="number">25</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rnn = nn_LSTM(vocab_size, hidden_size, vocab_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=<span class="number">0.005</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">X_batch, y_batch</span>):</span><br><span class="line">    h_prev = rnn.initHidden()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    batch_loss = torch.tensor(<span class="number">0</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_batch)):</span><br><span class="line">        y_score, h_prev = rnn(X_batch[i].view(<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>), h_prev)</span><br><span class="line">        loss = loss_fn(y_score.view(<span class="number">1</span>,-<span class="number">1</span>), y_batch[i].view(<span class="number">1</span>))</span><br><span class="line">        batch_loss += loss</span><br><span class="line">    batch_loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_score, batch_loss/<span class="built_in">len</span>(X_batch)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">f&#x27;logs/lstm1_<span class="subst">&#123;time.strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>准备好了吗？所有CUDA&#x2F;Tensor核心全部启动启动还有这个，启动训练！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">all_losses = []</span><br><span class="line">print_every = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> get_batch(X_train, y_train, seq_length):</span><br><span class="line">        X_batch, y_batch = batch</span><br><span class="line">        _, batch_loss = train(X_batch, y_batch)</span><br><span class="line">        all_losses.append(batch_loss.item())</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(all_losses)%print_every==<span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;----\n训练正在进行，请耐心等待 Loss:<span class="subst">&#123;np.mean(all_losses[-print_every:])&#125;</span> at iter: <span class="subst">&#123;<span class="built_in">len</span>(all_losses)&#125;</span>\n----&#x27;</span>)</span><br><span class="line">            <span class="comment"># log to tensorboard every X iterations. Can be removed if Tensorboard is not installed.</span></span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;loss&#x27;</span>, np.mean(all_losses[-<span class="number">100</span>:]), <span class="built_in">len</span>(all_losses))</span><br><span class="line">            <span class="comment"># generate text every X iterations</span></span><br><span class="line">            <span class="built_in">print</span>(sample_chars(rnn, X_batch[<span class="number">0</span>], rnn.initHidden(), <span class="number">200</span>))</span><br></pre></td></tr></table></figure>
<p>终于可以启动生成器了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sample_chars(rnn, X_batch[<span class="number">20</span>], rnn.initHidden(), <span class="number">200</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(rnn.state_dict(), <span class="string">&#x27;one.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>把模型存着。你自己训练的模型你自己可以随便搞，作者对此没有版权和责任<br>另外，推荐大家去看看某科学的超电磁炮，特别好看！要联系的去我的主页加我邮箱</p>

    </section>

    
    

    <section class="post-cc">
        <div class="post-cc-author">
            <span class="bold">作者：</span><a href="mailto:fatcathello@outlook.com" title="Steve Hu">Steve Hu</a>
        </div>
        <div class="post-cc-link">
            <span class="bold">链接：</span><a href="/2023/09/10/%E5%86%99%E5%B0%8F%E8%AF%B4%E6%9C%BA%E5%99%A8%E4%BA%BA/">https://lovehuiyuanai.github.io/2023/09/10/写小说机器人/</a>
        </div>
        <div class="post-cc-source">
            <span class="bold">来源：</span><a href="/">Steve Hu的博客</a>
        </div>
        <div class="ppost-cc-declare">
            <span class="bold">版权声明: </span>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a> 许可协议。著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 
        </div>
    </section>

    <section class="flex-between" style="margin: 20px 0;">
        <div class="pre">
            
                <a href="/2024/02/02/XIKO%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91%E8%B7%AF%E7%BA%BF%E7%A1%AE%E5%AE%9A/" class="change-link">
                    <span class="icon"><i class="fa fa-angle-double-left"></i></span>
                    <span class="text">
                        <span class="identify">上一篇</span>
                        <span class="text-title">XIKO技术开发路线确定</span>
                    </span>
                </a>
            
        </div>
        <div class="next">
            
                <a href="/2023/09/07/%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%EF%BC%8C%E4%BB%85%E5%8F%91%E5%B8%83%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE/" class="change-link">
                    <span class="text">
                        <span class="identify" style="text-align: right;">下一篇</span>
                        <span class="text-title" style="text-align: right;">加密博客，仅发布者可以访问</span>
                    </span>
                    <span class="icon"><i class="fa fa-angle-double-right"></i></span>
                </a>
            
        </div>
    </section>

    <!-- comment -->
    <section class="comment-container">
        <!-- giscus -->
        
    </section>
</article>




            <section class="article-bottom flex-between">
                <div class="flex-center">
                    <img src="/imgs/logo.png" class="logo">
                    <span>海纳百川有容乃大；壁立千仞无欲则刚</span>
                </div>
                <div class="flex-center">
                    <a href="mailto:fatcathello@outlook.com">
                        <span title="envelope">
                            <i class="fa fa-envelope" aria-hidden="true"></i>
                        </span>
                    </a>
                    <a target="_blank" rel="noopener" href="https://github.com/lovehuiyuanai">
                        <span title="github" style="font-size: 18px;">
                            <i class="fa fa-github" aria-hidden="true"></i>
                        </span>
                    </a>
                </div>
            </section>
        </div>
    </main>

    <div class="to-top">
    <a href="javascript: void(0)" onclick="delicate.toTop()">
        <i class="fa fa-chevron-up" aria-hidden="true"></i>
    </a>
</div>

    <footer>
        <div class="footer-info gray">
    <div>
        <span><a target="_blank" rel="noopener" href="https://github.com/can-dy-jack/hexo-theme-delicate">Delicate</a> theme designed with ❤️ by <a target="_blank" rel="noopener" href="https://github.com/can-dy-jack">can-dy-jack</a></span>
    </div>
    <div>
        <span>Copyright Ⓒ 2024. All rights reserved.</span>
    </div>
</div>
    </footer>

    


<script src="/js/index.js"></script>


</body>

</html>