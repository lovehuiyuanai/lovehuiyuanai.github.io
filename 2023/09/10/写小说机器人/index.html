<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>写小说机器人 | 哀猫的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="胡哲涵最新开源项目，小说大师python源码">
<meta property="og:type" content="article">
<meta property="og:title" content="写小说机器人">
<meta property="og:url" content="http://example.com/2023/09/10/%E5%86%99%E5%B0%8F%E8%AF%B4%E6%9C%BA%E5%99%A8%E4%BA%BA/index.html">
<meta property="og:site_name" content="哀猫的博客">
<meta property="og:description" content="胡哲涵最新开源项目，小说大师python源码">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-09-10T07:05:13.000Z">
<meta property="article:modified_time" content="2023-09-10T07:13:56.542Z">
<meta property="article:author" content="Shrry Hu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="哀猫的博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">哀猫的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">你指尖跃动的电光，是我此生不灭的信仰！唯我超电磁炮永世长存！</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-写小说机器人" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/09/10/%E5%86%99%E5%B0%8F%E8%AF%B4%E6%9C%BA%E5%99%A8%E4%BA%BA/" class="article-date">
  <time class="dt-published" datetime="2023-09-10T07:05:13.000Z" itemprop="datePublished">2023-09-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      写小说机器人
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>胡哲涵最新开源项目，小说大师python源码</p>
<span id="more"></span>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">                                                          声明</span></span><br><span class="line"><span class="code">本项目基于pytorch实现。可以自行更换训练文本，但请记住：训练文本一定一定得是TXT格式！！！</span></span><br><span class="line"><span class="code">另外，本项目仅用作学习交流，严禁用于商用、参加任何形式以及团体组织的比赛，请使用者严格遵守Apache License Version 开源协议</span></span><br><span class="line"><span class="code">作者不承担此程序造成的一切后果与责任。</span></span><br><span class="line"><span class="code">版权归作者重庆市小学生胡哲涵所有，Github likehuiyuanai。</span></span><br><span class="line"><span class="code">PS：最近喜欢看某科学的超电磁炮，所以实例代码用的是日本轻小说《魔法禁书目录》作为训练数据，暂拒绝公开此数据集（你可以自己整理，字数多亿些训练效果好点</span></span><br><span class="line"><span class="code">本博客（帖子）的版权也同样归胡哲涵所有，严禁转发或参加比赛！</span></span><br><span class="line"><span class="code">低调！低调！低调！</span></span><br></pre></td></tr></table></figure>
<p>正式开始（暂时没有开发代码生成器的意向，毕竟我不喜欢。没有bug的代码等同于人失去了灵魂！）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>导入库，幼儿园都学过吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./mfjsml.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.readlines()</span><br></pre></td></tr></table></figure>
<p>读取数据集，mfjsml这个名字可以改，但数据集和jupyter lab代码放一个文件夹下！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=<span class="string">&#x27;&#x27;</span>.join(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data[:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>
<p>展示数据集的一部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chars = <span class="built_in">list</span>(<span class="built_in">set</span>(data))</span><br><span class="line">data_size, vocab_size = <span class="built_in">len</span>(data), <span class="built_in">len</span>(chars)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;data has <span class="subst">&#123;data_size&#125;</span> characters, <span class="subst">&#123;vocab_size&#125;</span> unique.&#x27;</span>)</span><br><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125;</span><br><span class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars) &#125;</span><br></pre></td></tr></table></figure>
<p>接下来我们开始构建LSTM模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train = csr_matrix((<span class="built_in">len</span>(data), <span class="built_in">len</span>(chars)), dtype=np.<span class="built_in">int</span>)</span><br><span class="line">char_id = np.array([chars.index(c) <span class="keyword">for</span> c <span class="keyword">in</span> data])</span><br><span class="line">X_train[np.arange(<span class="built_in">len</span>(data)), char_id] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train = np.roll(char_id,-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">X_train, y_train, seq_length</span>):</span><br><span class="line">    X = X_train</span><br><span class="line">    <span class="comment">#X = torch.from_numpy(X_train).float()</span></span><br><span class="line">    y = torch.from_numpy(y_train).long()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(y), seq_length):   </span><br><span class="line">        id_stop = i+seq_length <span class="keyword">if</span> i+seq_length &lt; <span class="built_in">len</span>(y) <span class="keyword">else</span> <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">yield</span>([torch.from_numpy(X[i:id_stop].toarray().astype(np.float32)), </span><br><span class="line">               y[i:id_stop]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample_chars</span>(<span class="params">rnn, X_seed, h_prev, length=<span class="number">20</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Generate text using trained model&#x27;&#x27;&#x27;</span></span><br><span class="line">    X_next = X_seed</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):        </span><br><span class="line">            y_score, h_prev = rnn(X_next.view(<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>), h_prev)</span><br><span class="line">            y_prob = nn.Softmax(<span class="number">0</span>)(y_score.view(-<span class="number">1</span>)).detach().numpy()</span><br><span class="line">            y_pred = np.random.choice(chars,<span class="number">1</span>, p=y_prob).item()</span><br><span class="line">            results.append(y_pred)</span><br><span class="line">            X_next = torch.zeros_like(X_seed)</span><br><span class="line">            X_next[chars.index(y_pred)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(results)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">nn_LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size)</span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, hidden</span>):</span><br><span class="line">        _, hidden = self.lstm(X, hidden)</span><br><span class="line">        output = self.out(hidden[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">initHidden</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> (torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size),</span><br><span class="line">                torch.zeros(<span class="number">1</span>, <span class="number">1</span>, self.hidden_size)</span><br><span class="line">               )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hidden_size = <span class="number">256</span></span><br><span class="line">seq_length = <span class="number">25</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rnn = nn_LSTM(vocab_size, hidden_size, vocab_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=<span class="number">0.005</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">X_batch, y_batch</span>):</span><br><span class="line">    h_prev = rnn.initHidden()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    batch_loss = torch.tensor(<span class="number">0</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X_batch)):</span><br><span class="line">        y_score, h_prev = rnn(X_batch[i].view(<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>), h_prev)</span><br><span class="line">        loss = loss_fn(y_score.view(<span class="number">1</span>,-<span class="number">1</span>), y_batch[i].view(<span class="number">1</span>))</span><br><span class="line">        batch_loss += loss</span><br><span class="line">    batch_loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_score, batch_loss/<span class="built_in">len</span>(X_batch)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(<span class="string">f&#x27;logs/lstm1_<span class="subst">&#123;time.strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>准备好了吗？所有CUDA&#x2F;Tensor核心全部启动启动还有这个，启动训练！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">all_losses = []</span><br><span class="line">print_every = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> get_batch(X_train, y_train, seq_length):</span><br><span class="line">        X_batch, y_batch = batch</span><br><span class="line">        _, batch_loss = train(X_batch, y_batch)</span><br><span class="line">        all_losses.append(batch_loss.item())</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(all_losses)%print_every==<span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;----\n训练正在进行，请耐心等待 Loss:<span class="subst">&#123;np.mean(all_losses[-print_every:])&#125;</span> at iter: <span class="subst">&#123;<span class="built_in">len</span>(all_losses)&#125;</span>\n----&#x27;</span>)</span><br><span class="line">            <span class="comment"># log to tensorboard every X iterations. Can be removed if Tensorboard is not installed.</span></span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;loss&#x27;</span>, np.mean(all_losses[-<span class="number">100</span>:]), <span class="built_in">len</span>(all_losses))</span><br><span class="line">            <span class="comment"># generate text every X iterations</span></span><br><span class="line">            <span class="built_in">print</span>(sample_chars(rnn, X_batch[<span class="number">0</span>], rnn.initHidden(), <span class="number">200</span>))</span><br></pre></td></tr></table></figure>
<p>终于可以启动生成器了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sample_chars(rnn, X_batch[<span class="number">20</span>], rnn.initHidden(), <span class="number">200</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(rnn.state_dict(), <span class="string">&#x27;one.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>把模型存着。你自己训练的模型你自己可以随便搞，作者对此没有版权和责任<br>另外，推荐大家去看看某科学的超电磁炮，特别好看！要联系的去我的主页加我邮箱</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/09/10/%E5%86%99%E5%B0%8F%E8%AF%B4%E6%9C%BA%E5%99%A8%E4%BA%BA/" data-id="clmiwzbg10008u8dm38echw8y" data-title="写小说机器人" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/09/14/%E7%82%AE%E5%A7%90%E5%A5%BD%E5%8F%AF%E7%88%B1%EF%BC%81/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          炮姐好可爱！
        
      </div>
    </a>
  
  
    <a href="/2023/09/07/%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%EF%BC%8C%E4%BB%85%E5%8F%91%E5%B8%83%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">加密博客，仅发布者可以访问</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2/" rel="tag">加密博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%B4%AB%E9%B2%B8/" rel="tag">紫鲸</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2/" style="font-size: 10px;">加密博客</a> <a href="/tags/%E7%B4%AB%E9%B2%B8/" style="font-size: 10px;">紫鲸</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/09/14/%E7%82%AE%E5%A7%90%E5%A5%BD%E5%8F%AF%E7%88%B1%EF%BC%81/">炮姐好可爱！</a>
          </li>
        
          <li>
            <a href="/2023/09/10/%E5%86%99%E5%B0%8F%E8%AF%B4%E6%9C%BA%E5%99%A8%E4%BA%BA/">写小说机器人</a>
          </li>
        
          <li>
            <a href="/2023/09/07/%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%EF%BC%8C%E4%BB%85%E5%8F%91%E5%B8%83%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE/">加密博客，仅发布者可以访问</a>
          </li>
        
          <li>
            <a href="/2023/08/31/%E8%83%A1%E5%93%B2%E6%B6%B5%E5%90%8D%E8%A8%80%E7%9B%AE%E5%BD%95/">胡哲涵名言目录</a>
          </li>
        
          <li>
            <a href="/2023/08/20/ChatGLM%E7%9A%84CPU%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/">ChatGLM的CPU推理方案</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 Shrry Hu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>