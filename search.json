[{"title":"2024年的一些计划","url":"/2024/03/09/2024%E5%B9%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%A1%E5%88%92/","content":"<h1 id=\"XIKO\"><a href=\"#XIKO\" class=\"headerlink\" title=\"XIKO\"></a>XIKO</h1><p>正如你所见，XIKO</p>\n","tags":["计划"]},{"title":"ChatGLM的CPU推理方案","url":"/2023/08/20/ChatGLM%E7%9A%84CPU%E6%8E%A8%E7%90%86%E6%96%B9%E6%A1%88/","content":"<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoTokenizer, AutoModel</span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"string\">&quot;THUDM/chatglm2-6b-int4&quot;</span>, trust_remote_code=<span class=\"literal\">True</span>)</span><br><span class=\"line\">model = AutoModel.from_pretrained(<span class=\"string\">&quot;THUDM/chatglm2-6b-int4&quot;</span>, trust_remote_code=<span class=\"literal\">True</span>).half().<span class=\"built_in\">float</span>()</span><br><span class=\"line\">model = model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">response, history = model.chat(tokenizer, <span class=\"string\">&quot;写一首赞美雪的现代诗&quot;</span>, history=[])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(response)</span><br></pre></td></tr></table></figure>\n"},{"title":"XIKO技术开发路线确定","url":"/2024/02/02/XIKO%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91%E8%B7%AF%E7%BA%BF%E7%A1%AE%E5%AE%9A/","content":"<p>XIKO：为青少年最后的秘密而战！</p>\n<span id=\"more\"></span>\n<p>XIKO前端正式确定使用HTML、CSS开发，后端采用C++开发，数据库使用json文件。服务器正式确定使用Linux操作系统<br>Q&amp;A<br>Q：为什么使用C++作为后端开发语言？<br>A：因为XIKO立项时我只会C++，学习Js太耗时间了，我计划继续精进C++，尽快搞定后端系统。</p>\n<p>Q：XIKO后续会继续研发Js后端与传统数据库架构系统吗？<br>A：XIKO在V1.0.6版本会更新Js后端版本，OpenXIKO在V1.1.1版本会加入对Js后端的支持。数据库系统XIKO暂时没有更新计划（数据库不符合XIKO对于简洁之上的理念）。</p>\n<p>（附XIKO更新计划表）<br>2024.2.15 完成XIKO V1.0.0版本C++后端开发。<br>2024.2.16 完成XIKO V1.0.0版本HTML前端开发。<br>2024.7.20 完成XIKO服务器测试。<br>2024.8.11 完成OpenXIKO V1.0.0版本上线Github、Gitee。</p>\n<p>XIKO V1.0.0 完成基本轻量论坛系统开发。<br>XIKO V1.0.2 对XIKO移动网页端支持。<br>XIKO V1.0.4 对XIKO页面进行更新，做到美观与简洁的提升。<br>XIKO V1.0.6 XIKO后端支持Js，方便后期项目接手。</p>\n<p>OpenXIKO V1.0.0 正式发布XIKO开源版本。<br>OpenXIKO V1.0.6 与Xiko V1.0.2更新对齐。<br>OpenXIKO V1.0.7 继续完成OpenXIKO API对齐编写。<br>OpenXIKO V1.1.1 与XIKO V1.0.6更新对齐。<br>……<br>后续开发将持续公开。</p>\n<p>Q：XIKO还会开发哪些配套产品？<br>A：平方云系统，保证XIKO产品信息传递安全，提升用户体验。喵的书架，是一个小说发布网站。喵的日记，从用户开始编写日记到服务器保存日记全程采用高安全性算法，让用户的隐私不再是纸上谈兵。XIKOGPT，XIKO的ai大模型产品，目前还不能公布太多消息，敬请期待！</p>\n<p>Q：XIKO平台有类似小天才的内容审核机制吗？<br>A：XIKO平台只会对于极端、色情、暴力、人身攻击的不良言论进行查封，尽可能保证用户的观点自由。</p>\n","tags":["OpenXIKO"]},{"title":"XIKOTODAY正式启用python后端","url":"/2024/03/24/XIKOTODAY%E6%AD%A3%E5%BC%8F%E5%90%AF%E7%94%A8python%E5%90%8E%E7%AB%AF/","content":"<p>哈哈哈哈……</p>\n<p>我真是服了我自己了，两个文件夹的urls写成一个去了，反复导入</p>\n<p>Django:大哥，你这怎么重复导入urls文件啊</p>\n<p>我:什么？***！</p>\n<p>总之以后注意吧</p>\n","tags":["开发小趣事"]},{"title":"NEW","url":"/2023/07/23/NEW/","content":""},{"title":"test_my_site","url":"/2023/07/21/test-my-site/","content":""},{"title":"OpenXiko介绍文档","url":"/2023/01/28/blog/","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"9caa2a154b01a2d479ab3b1539d499e62ab43e877e93d52d2e2f0938eaacc488\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","tags":["小胡的开源项目"]},{"title":"world","url":"/2023/07/14/world-1/","content":""},{"title":"world","url":"/2023/07/22/world/","content":""},{"title":"一些思考","url":"/2023/08/14/%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/","content":"<p>人的大脑运作，可以看作是一个函数系统，每个函数，都对应一个输入，产生一个输出。</p>\n<span id=\"more\"></span>\n<p>如果按照这个原理，编写一个用于批量处理此类函数的机器学习程序，不是几句可以仿照人脑进行思考了吗？由此，我决定在此宣布，仿人类数字系统计划，即ADSP，正式启动！我将在此Blog中一直不断更新。</p>\n"},{"title":"加密博客，仅发布者可以访问","url":"/2023/09/07/%E5%8A%A0%E5%AF%86%E5%8D%9A%E5%AE%A2%EF%BC%8C%E4%BB%85%E5%8F%91%E5%B8%83%E8%80%85%E5%8F%AF%E4%BB%A5%E8%AE%BF%E9%97%AE/","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"fac2f243a6927c1bde5d4ac2751d8c0be979eed0942cf9d2032f11b5a2ce504a\">fae7d0b5377b9f6c290dcd8adf41b25381416438debc227eee11d0ec2314c232463928add14c62c788a24a18da5f5941753b30bc6525f02b6b098ce6a14c253da62f446c0609b22536c687c48d510515bdc5b56bb459aeab00d93b516ce93ed711045decc7eff2e2e351339ef72c66a18f78cf6ee2352e5d6ffe06a94eab4ee143193b64ca0b82ae52a67dabe88d5444f42fd00de8b4c5b8b1a56a04fda6b8a8e067269b1dfad1cd028d4fd145306515</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","tags":["加密博客"]},{"title":"中文测试一","url":"/2023/08/14/%E4%B8%AD%E6%96%87%E6%B5%8B%E8%AF%95%E4%B8%80/","content":""},{"title":"写小说机器人","url":"/2023/09/10/%E5%86%99%E5%B0%8F%E8%AF%B4%E6%9C%BA%E5%99%A8%E4%BA%BA/","content":"<p>胡哲涵最新开源项目，小说大师python源码</p>\n<span id=\"more\"></span>\n<figure class=\"highlight markdown\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"code\">                                                          声明</span></span><br><span class=\"line\"><span class=\"code\">本项目基于pytorch实现。可以自行更换训练文本，但请记住：训练文本一定一定得是TXT格式！！！</span></span><br><span class=\"line\"><span class=\"code\">另外，本项目仅用作学习交流，严禁用于商用、参加任何形式以及团体组织的比赛，请使用者严格遵守Apache License Version 开源协议</span></span><br><span class=\"line\"><span class=\"code\">作者不承担此程序造成的一切后果与责任。</span></span><br><span class=\"line\"><span class=\"code\">版权归作者重庆市小学生胡哲涵所有，Github likehuiyuanai。</span></span><br><span class=\"line\"><span class=\"code\">PS：最近喜欢看某科学的超电磁炮，所以实例代码用的是日本轻小说《魔法禁书目录》作为训练数据，暂拒绝公开此数据集（你可以自己整理，字数多亿些训练效果好点</span></span><br><span class=\"line\"><span class=\"code\">本博客（帖子）的版权也同样归胡哲涵所有，严禁转发或参加比赛！</span></span><br><span class=\"line\"><span class=\"code\">低调！低调！低调！</span></span><br></pre></td></tr></table></figure>\n<p>正式开始（暂时没有开发代码生成器的意向，毕竟我不喜欢。没有bug的代码等同于人失去了灵魂！）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">from</span> scipy.sparse <span class=\"keyword\">import</span> csr_matrix</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorboardX <span class=\"keyword\">import</span> SummaryWriter</span><br><span class=\"line\">%matplotlib inline</span><br></pre></td></tr></table></figure>\n<p>导入库，幼儿园都学过吧。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;./mfjsml.txt&#x27;</span>, <span class=\"string\">&#x27;r&#x27;</span>, encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    data = f.readlines()</span><br></pre></td></tr></table></figure>\n<p>读取数据集，mfjsml这个名字可以改，但数据集和jupyter lab代码放一个文件夹下！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">data=<span class=\"string\">&#x27;&#x27;</span>.join(data)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(data[:<span class=\"number\">100</span>])</span><br></pre></td></tr></table></figure>\n<p>展示数据集的一部分</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">chars = <span class=\"built_in\">list</span>(<span class=\"built_in\">set</span>(data))</span><br><span class=\"line\">data_size, vocab_size = <span class=\"built_in\">len</span>(data), <span class=\"built_in\">len</span>(chars)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;data has <span class=\"subst\">&#123;data_size&#125;</span> characters, <span class=\"subst\">&#123;vocab_size&#125;</span> unique.&#x27;</span>)</span><br><span class=\"line\">char_to_ix = &#123; ch:i <span class=\"keyword\">for</span> i,ch <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(chars) &#125;</span><br><span class=\"line\">ix_to_char = &#123; i:ch <span class=\"keyword\">for</span> i,ch <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(chars) &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来我们开始构建LSTM模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">X_train = csr_matrix((<span class=\"built_in\">len</span>(data), <span class=\"built_in\">len</span>(chars)), dtype=np.<span class=\"built_in\">int</span>)</span><br><span class=\"line\">char_id = np.array([chars.index(c) <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> data])</span><br><span class=\"line\">X_train[np.arange(<span class=\"built_in\">len</span>(data)), char_id] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">y_train = np.roll(char_id,-<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">X_train.shape</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">y_train.shape</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_batch</span>(<span class=\"params\">X_train, y_train, seq_length</span>):</span><br><span class=\"line\">    X = X_train</span><br><span class=\"line\">    <span class=\"comment\">#X = torch.from_numpy(X_train).float()</span></span><br><span class=\"line\">    y = torch.from_numpy(y_train).long()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(y), seq_length):   </span><br><span class=\"line\">        id_stop = i+seq_length <span class=\"keyword\">if</span> i+seq_length &lt; <span class=\"built_in\">len</span>(y) <span class=\"keyword\">else</span> <span class=\"built_in\">len</span>(y)</span><br><span class=\"line\">        <span class=\"keyword\">yield</span>([torch.from_numpy(X[i:id_stop].toarray().astype(np.float32)), </span><br><span class=\"line\">               y[i:id_stop]])</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sample_chars</span>(<span class=\"params\">rnn, X_seed, h_prev, length=<span class=\"number\">20</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;Generate text using trained model&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    X_next = X_seed</span><br><span class=\"line\">    results = []</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(length):        </span><br><span class=\"line\">            y_score, h_prev = rnn(X_next.view(<span class=\"number\">1</span>,<span class=\"number\">1</span>,-<span class=\"number\">1</span>), h_prev)</span><br><span class=\"line\">            y_prob = nn.Softmax(<span class=\"number\">0</span>)(y_score.view(-<span class=\"number\">1</span>)).detach().numpy()</span><br><span class=\"line\">            y_pred = np.random.choice(chars,<span class=\"number\">1</span>, p=y_prob).item()</span><br><span class=\"line\">            results.append(y_pred)</span><br><span class=\"line\">            X_next = torch.zeros_like(X_seed)</span><br><span class=\"line\">            X_next[chars.index(y_pred)] = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&#x27;&#x27;</span>.join(results)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">nn_LSTM</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size, output_size</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.hidden_size = hidden_size</span><br><span class=\"line\">        self.lstm = nn.LSTM(input_size, hidden_size)</span><br><span class=\"line\">        self.out = nn.Linear(hidden_size, output_size)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, X, hidden</span>):</span><br><span class=\"line\">        _, hidden = self.lstm(X, hidden)</span><br><span class=\"line\">        output = self.out(hidden[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, hidden</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">initHidden</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> (torch.zeros(<span class=\"number\">1</span>, <span class=\"number\">1</span>, self.hidden_size),</span><br><span class=\"line\">                torch.zeros(<span class=\"number\">1</span>, <span class=\"number\">1</span>, self.hidden_size)</span><br><span class=\"line\">               )</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">hidden_size = <span class=\"number\">256</span></span><br><span class=\"line\">seq_length = <span class=\"number\">25</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">rnn = nn_LSTM(vocab_size, hidden_size, vocab_size)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">optimizer = torch.optim.Adam(rnn.parameters(), lr=<span class=\"number\">0.005</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">X_batch, y_batch</span>):</span><br><span class=\"line\">    h_prev = rnn.initHidden()</span><br><span class=\"line\">    optimizer.zero_grad()</span><br><span class=\"line\">    batch_loss = torch.tensor(<span class=\"number\">0</span>, dtype=torch.<span class=\"built_in\">float</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(X_batch)):</span><br><span class=\"line\">        y_score, h_prev = rnn(X_batch[i].view(<span class=\"number\">1</span>,<span class=\"number\">1</span>,-<span class=\"number\">1</span>), h_prev)</span><br><span class=\"line\">        loss = loss_fn(y_score.view(<span class=\"number\">1</span>,-<span class=\"number\">1</span>), y_batch[i].view(<span class=\"number\">1</span>))</span><br><span class=\"line\">        batch_loss += loss</span><br><span class=\"line\">    batch_loss.backward()</span><br><span class=\"line\">    optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y_score, batch_loss/<span class=\"built_in\">len</span>(X_batch)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">writer = SummaryWriter(<span class=\"string\">f&#x27;logs/lstm1_<span class=\"subst\">&#123;time.strftime(<span class=\"string\">&quot;%Y%m%d-%H%M%S&quot;</span>)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>准备好了吗？所有CUDA&#x2F;Tensor核心全部启动启动还有这个，启动训练！</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">all_losses = []</span><br><span class=\"line\">print_every = <span class=\"number\">100</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">20</span>):    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> batch <span class=\"keyword\">in</span> get_batch(X_train, y_train, seq_length):</span><br><span class=\"line\">        X_batch, y_batch = batch</span><br><span class=\"line\">        _, batch_loss = train(X_batch, y_batch)</span><br><span class=\"line\">        all_losses.append(batch_loss.item())</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(all_losses)%print_every==<span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;----\\n训练正在进行，请耐心等待 Loss:<span class=\"subst\">&#123;np.mean(all_losses[-print_every:])&#125;</span> at iter: <span class=\"subst\">&#123;<span class=\"built_in\">len</span>(all_losses)&#125;</span>\\n----&#x27;</span>)</span><br><span class=\"line\">            <span class=\"comment\"># log to tensorboard every X iterations. Can be removed if Tensorboard is not installed.</span></span><br><span class=\"line\">            writer.add_scalar(<span class=\"string\">&#x27;loss&#x27;</span>, np.mean(all_losses[-<span class=\"number\">100</span>:]), <span class=\"built_in\">len</span>(all_losses))</span><br><span class=\"line\">            <span class=\"comment\"># generate text every X iterations</span></span><br><span class=\"line\">            <span class=\"built_in\">print</span>(sample_chars(rnn, X_batch[<span class=\"number\">0</span>], rnn.initHidden(), <span class=\"number\">200</span>))</span><br></pre></td></tr></table></figure>\n<p>终于可以启动生成器了</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(sample_chars(rnn, X_batch[<span class=\"number\">20</span>], rnn.initHidden(), <span class=\"number\">200</span>))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">torch.save(rnn.state_dict(), <span class=\"string\">&#x27;one.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>把模型存着。你自己训练的模型你自己可以随便搞，作者对此没有版权和责任<br>另外，推荐大家去看看某科学的超电磁炮，特别好看！要联系的去我的主页加我邮箱</p>\n"},{"title":"新主题代码块测试","url":"/2024/02/19/%E6%96%B0%E4%B8%BB%E9%A2%98%E4%BB%A3%E7%A0%81%E5%9D%97%E6%B5%8B%E8%AF%95/","content":"<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"></span><br><span class=\"line\">mnist = tf.keras.datasets.mnist</span><br><span class=\"line\"></span><br><span class=\"line\">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class=\"line\">x_train, x_test = x_train / <span class=\"number\">255.0</span>, x_test / <span class=\"number\">255.0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model = tf.keras.models.Sequential([</span><br><span class=\"line\">  tf.keras.layers.Flatten(input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>)),</span><br><span class=\"line\">  tf.keras.layers.Dense(<span class=\"number\">128</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">  tf.keras.layers.Dropout(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">  tf.keras.layers.Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>)</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;adam&#x27;</span>,loss=<span class=\"string\">&#x27;sparse_categorical_crossentropy&#x27;</span>,metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model.fit(x_train, y_train, epochs=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">model.evaluate(x_test,  y_test, verbose=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>"},{"title":"紫鲸第一、二章","url":"/2023/08/15/%E7%B4%AB%E9%B2%B8%E7%AC%AC%E4%B8%80%E7%AB%A0/","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"75f843456653712d8cb1d41e7364813b35bfa4d4cb723d05d135f5c0e14eaab3\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/lib/hbe.js\"></script><link href=\"/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","tags":["紫鲸"]},{"title":"水一次","url":"/2024/06/18/%E6%B0%B4%E4%B8%80%E6%AC%A1/","content":""},{"title":"胡哲涵名言目录","url":"/2023/08/31/%E8%83%A1%E5%93%B2%E6%B6%B5%E5%90%8D%E8%A8%80%E7%9B%AE%E5%BD%95/","content":"<p>趁时光仍未忘记我之时，赶紧记下来。</p>\n<span id=\"more\"></span>\n<p>不用因为没看到晚霞愁眉苦脸，以后还会有。———《紫鲸 陆(别名风山篇第2章)》胡哲涵 著<br>街道上的灯光是纸上跃动的墨水。———胡哲涵作文《晚上的重庆》原稿因被撕破目前只剩残缺的三分之一。<br>所有人的名字就像时间这条大桥上的一个过客，他记不得。———我在知乎关于人生话题下的回答。<br>鸟不会在意时间，因为时间也不会在意它———《紫鲸 伍(别名紫色的鸟儿)》 胡哲涵 著</p>\n"},{"title":"第一个博客","url":"/2023/08/14/%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2/","content":"<blockquote>\n<p>这是我发布的基于HEXO的2.0版本博客，稍后我会写出教程</p>\n</blockquote>\n"}]